\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{epsfig}
\usepackage[tight]{subfigure}
\usepackage{comment}

% TD (Nitish)
%   online policy evaluation
%   learning rate, intuition and bounds (Mike)
% SARSA (Nitish)
%   define the loss function
%   derive the full gradient 
%   drop the extra term
%   explain that this is NOT gradient descent
%   mention the Bellman-Residual method
% Q-Learning (Mike)
%   online fitted Q-iteration
%   restate the parameter update
%   exploration policies: epsilon-greedy and Boltzman
% Off-Policy vs On-Policy learning (Mike)
% Experience Replay (Mike)


%extra packages added
%\usepackage{algorithm}
%\usepackage{algorithmic}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\uniform}{uniform}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {#1} \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[5]{\handout{#1}{#2}{#3}{#4}{#5}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

%\renewcommand{\baselinestretch}{1.25}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document header: change the lecture number, date, scribe name, and
% lecture topic.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\lecture{Statistical Techniques in Robotics (16-831, F10)}
%{Lecture\#NN (Thursday August 26)}
%{Lecturer: Drew Bagnell}
%{Scribe:YOUR\_NAME\_HERE \footnotemark}
%{Lecture Topic}

\lecture{Statistical Techniques in Robotics (16-899, S14)}%
{Lecture \#09, (Feb. 11, 2014)}%
{Lecturer: Drew Bagnell}%
{Scribes: Michael Koval and Nitish Thatte}%
{Q-Learning}

% Be sure to acknowledge all previous scribes.
\footnotetext[1]{Some content adapted from previous scribe: Jackie Libby}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document body: Scribe your notes here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the last few lectures we covered several offline batch methods such as Fitted Value Iteration, Fitted Q-Iteration, and Approximate Policy Iteration.  While these methods make efficient use of available training data, they may suffer due to their potentially high memory consumption. In this lecture, we present online techniques that can perform well even when presented with large amounts of data, as in the case of samples generated by simulations. With these online techniques, we perform an update after sampling each data tuple.  The following block diagram generalizes this idea.

\begin{figure}[h!]
	\centering
	\includegraphics[width=.4\columnwidth]{./images/figBlock}
	%\caption{}
	\label{fig.figBlock}
\end{figure}

First we will present the Temporal Differencing method for online updates in the context of policy evaluation. Then, we will look at approximate methods such as SARSA and Q-learning.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Temporal Differencing (TD)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: Explain that Eqn. 2.1 is policy evaluation, not TD.

When performing policy evaluation we would like,
\begin{equation}
	V^\pi(s) = \sum_{t = 0}^\infty \gamma^t E[r(s_t,\pi(s_t))]
	\label{eq.policyEval}
\end{equation}
where samples $s_t$ are drawn from the transition function following policy $\pi$. Given an estimate of the value function $\tilde{V}^\pi(s)$ we would like to perform an update in order to minimize the loss
\begin{equation}
	\mathcal{L} = \frac{1}{2} \left(V^\pi(s) - \tilde{V}^\pi(s) \right)^2.
	\label{eq.valLoss}
\end{equation}

Since we do not yet know the value function, evaluating this loss requires evaluation equation~\ref{eq.policyEval}. Therefore, this method requires waiting until the end of an episode before updating $\tilde{V}^\pi(s)$. To perform this update online given training tuples $(s,r,s')$, we can instead estimate $V^\pi(s)$ as $y = r + \gamma \tilde{V}^\pi(s')$. Plugging this estimate into the loss function we get
\begin{equation}
	\mathcal{L} = \frac{1}{2} \left(y - \tilde{V}^\pi(s) \right)^2.
	\label{eq.valLossApprox}
\end{equation}

Differentiating this loss gives
\begin{equation}
    \nabla_{\tilde{V}^\pi}\mathcal{L} = \left(y - \tilde{V}^\pi(s) \right) \left(\nabla_{\tilde{V}^\pi} y - 1 \right)
    \label{eq.valGrad}
\end{equation}
It is not possible to get unbiased estimates of both $\tilde{V}_\pi(s)$ and $y$ with only one $(s,r,s')$ tuple. To avoid this issue, the TD method ignores the $\nabla_{\tilde{V}^\pi} y$ term yielding the following update rule:
\begin{align}
    \tilde{V}^\pi(s) &\leftarrow  \tilde{V}^\pi - \alpha \nabla_{\tilde{V}^\pi}\mathcal{L} \notag \\
    \tilde{V}^\pi(s) &\leftarrow  \tilde{V}^\pi + \alpha \left(r + \gamma \tilde{V}^\pi(s') - \tilde{V}^\pi(s) \right) \notag \\
    \tilde{V}^\pi(s) &\leftarrow  (1 - \alpha) \tilde{V}^\pi(s) + \alpha \left(r+ \gamma \tilde{V}^\pi(s') \right). 
    \label{eq.valUpdate}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Algorithm TD:}

\begin{enumerate}
	\item Initialize $\tilde{V}^\pi$
	\item $\forall s$, repeat until $\tilde{V}^\pi(s)$ converges
	\begin{enumerate}
		\item Initialize $s$
		\item Repeat until $s$ is terminal (This is a trajectory from a particular intitial starting state.)
		\begin{enumerate}
			\item Do $\pi(s)$
			\item Observe $s', r$
			\item Update $\tilde{V}^\pi(s)$ given  $(s, r, s')$
			\label{step.updateTD}
			\item $s \leftarrow s'$
		\end{enumerate}		
	\end{enumerate}		
\end{enumerate}		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Grid-based world example:}
The diagram below shows a grid-based world, where the robot starts in the upper left, and the goal is in the lower right.  The robot gets a reward of $+1$ if it reaches the goal, and $0$ everywhere else.  There is a discount factor of $\gamma$.  The policy is for the robot to go right until it reaches the wall, and then go down.
\begin{figure}[h!]
	\centering
	\includegraphics[width=.4\columnwidth]{./images/fig1}
	%\caption{}
	\label{fig.fig1}
\end{figure}

We start by initializing all states to 0:

\begin{figure}[h!]
	\centering
	\includegraphics[width=.4\columnwidth]{./images/fig2}
	%\caption{}
	\label{fig.fig2}
\end{figure}

As the robot moves one cell over from the start state (yellow arrow above), the reward is $0$, and the value of both the current state and the next state is $0$, so the gradient used in the update rule (\cref{eq.valUpdate}) evaluates to $0$ and no update is performed. As the robot moves into the goal state (red arrow), the reward is $1$, so the gradient evaluates to $1$.  We then update the second-to-last cell with \cref{eq.valUpdate} and we get:

\begin{figure}[h!]
	\centering
	\includegraphics[width=.4\columnwidth]{./images/fig3}
	%\caption{}
	\label{fig.fig3}
\end{figure}

Another iteration of the algorithm gives us:

\begin{figure}[h!]
	\centering
	\includegraphics[width=.4\columnwidth]{./images/fig4}
	%\caption{}
	\label{fig.fig4}
\end{figure}

This method is slow, because we have to run the whole policy just to update the next cell.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SARSA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
SARSA extends the Temporal Differencing method presented in the previous section to evaluate policies using state-action functions $Q^\pi(s,a)$. Similar to the TD case, we wish to evaluate a policy by performing an online update to obtain an estimate, $\tilde{Q}^\pi(s,a)$, of the true state-action function $Q^\pi(s,a)$, which satisfies:
\begin{equation}
	Q^\pi(s,a) = r(s,a) + \sum_{t = 1}^\infty \gamma^t E[r(s_t,\pi(s_t))]
	\label{eq.sarsaEval}
\end{equation}

As in TD, we seek to minimize the loss
\begin{equation}
	\mathcal{L} = \frac{1}{2} \left(y - \tilde{Q}^\pi(s,a) \right)^2
	\label{eq.sarsaLoss}
\end{equation}
where $y = r(s,a) + \gamma \tilde{Q}^\pi(s',\pi(s'))$. Following a similar derivation as used for the TD update, we arive at the SARSA update rule:


\begin{equation}
    \tilde{Q}^\pi(s,a) \leftarrow  (1 - \alpha) \tilde{Q}^\pi(s,a) + \alpha \left(r(s,a) + \gamma \tilde{Q}^\pi(s',\pi(s')) \right). 
	\label{eq.sarsaUpdate}
\end{equation}
\subsection*{Algorithm SARSA:}
\begin{enumerate}
	\item Initialize $Q^\pi$
	\item Repeat until $Q^\pi$ converges
	\begin{enumerate}
		\item Initialize $s$, $a \sim \hat{\pi}_{Q(s)}$

		(This means we initialize the action randomly according to some distribution $\hat{\pi}_{Q(s)}$.)
		\item Repeat until $s$ is terminal
		\begin{enumerate}
			\item Do $a$
			\item Observe $s'$, $r$
			\item Pick $a' \sim \hat{\pi}_{Q(s')}$
			\item Update $Q^\pi(s,a)$ given $(s, a, r, s', a')$
			
			(This is where SARSA comes from!)
			\item $s \leftarrow s'$, $a \leftarrow a'$ 
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{defining $\hat{\pi}_Q$}
There are two ways that $\hat{\pi}_Q$ can be defined:
\begin{enumerate}
	\item $\varepsilon$ - greedy $\hat{\pi}_Q$
		\begin{equation}
			\hat{\pi}_Q(s) = 
			\begin{cases}
				\operatorname{arg\,max}_{a}Q(s,a) & \text{with probability } 1 - \varepsilon, \\
				\text{random action} & \text{with probability } \varepsilon.
			\end{cases}
		\end{equation}
	\item Boltzmann exploration
		\begin{equation}
			\hat{\pi}_Q(s,a) =\frac{\exp(\frac{Q(s,a)}{B})}{\sum_{a'}\exp(\frac{Q(s,a')}{B})}
		\end{equation}
		The denominator is a normalization factor.  If the $Q$ value of a certain action is high, then we pick that action with higher probability.  $B$ is a temperature term.  The higher the temperature, the closer to a uniform distribution, and so the less greedy.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection*{SARSA Update with Function Approximator}
For the parametric case, we change Eq. (\ref{eq.sarsaUpdate}) to:
\begin{equation}
	\theta \leftarrow \theta + \alpha \delta \bigtriangledown_\theta Q_\theta(s,a)
\end{equation}
For example, if $Q_\theta(s,a) = \theta^T f(s,a)$, then $\bigtriangledown_\theta Q_\theta(s,a) = f(s,a)$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Algorithm SARSA($\lambda$):}
(SARSA with elibility trace)
\begin{enumerate}
	\item Initialize $Q$
	\item Repeat until $Q$ converges
	\begin{enumerate}
		\item Initialize $s$, $a \sim \hat{\pi}_\theta(s)$
		\item Repeat until $s$ is terminal
		\begin{enumerate}
			\item Do $a$
			\item Observe $s'$, $r$
			\item Pick $a' \sim \hat{\pi}_{Q(s')}$
			\item $e(s,a) \leftarrow e(s,a) + 1$
			\item $\delta \leftarrow (r + \gamma Q(s',a') - Q(s,a))$
			\item $\forall (s",a")$
			\begin{enumerate}
				\item $Q(s",a") \leftarrow Q(s",a") + \alpha \delta e(s",a")$
				\item $e(s",a") \leftarrow \lambda \gamma e(s",a")$	
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
\end{enumerate}
\end{comment}

% Q-Learning (Mike)
%   online fitted Q-iteration
%   restate the parameter update
%   exploration policies: epsilon-greedy and Boltzman
\section{Q-Learning}
Q-Learning attempts to learn the optimal action-value function\footnote{This is
sometimes written as $Q^*(s, a)$.} $Q(s, a)$ from an online stream of
experiences. Suppose we receive experience $(s, a, r, s')$ with the approximate
action-value function $\tilde{Q}(s, a)$. Performing a Bellman backup would
update the action-value function
\begin{align*}
    \tilde{Q}(s, a) \gets r + \gamma \max_{a' \in A} \tilde{Q}(s', a').
\end{align*}

However, just as in SARSA, this performs poorly when the transition or reward
functions are stochastic. Instead, we update $\tilde{Q}^*$ to the weighted sum
\begin{align*}
    \tilde{Q}(s, a) \gets \alpha \left[r + \gamma \max_{a' \in A} \tilde{Q}(s', a')\right]
                          + (1 - \alpha) \tilde{Q}(s, a)
\end{align*}
where $0 \le \alpha \le 1$ is the \emph{learning rate} parameter.

\subsection{Fitted Q-Learning}
Just as the fitted Q-iteration algorithm, we can use a function approximator to
approximate the action-value function. Suppose that we approximate $Q$ with the
the function $\tilde{Q}^\theta$ with hyperparameter $\theta$. Instead of
directly updating our action-value function, we now must update $\theta$ to
achieve the desired change in $\tilde{Q}^\theta$.

To fit $\theta$, we minimize a loss function
\begin{align*}
    \mathcal{L} &= \frac{1}{2} \left( y - Q^\theta(s, a) \right)^2
\end{align*}
that penalizes deviation between the approximate action-value function
$Q^\theta(s, a)$ and the value $y = r + \gamma \max_{a' \in A} Q^\theta(s',
a')$ predicted by a Bellman backup. We will now find the optimal parameter
$\theta$ by performing gradient descent on $\mathcal{L}$ with the update rule
\begin{align}
    \theta &\gets (1 - \alpha) \theta + \alpha \nabla_\theta \mathcal{L}
    \label{eqn:qlearning-step}
\end{align}

First, we must derive the gradient of $\mathcal{L}$. By applying the chain
rule, we find
\begin{align*}
    \nabla_\theta \mathcal{L} &= \left(y - Q^\theta(s, a)\right) \left[
                                      \nabla_\theta y
                                    - \nabla_\theta Q^\theta(s, a) \right] \\
                              &= \left(y - Q^\theta(s, a)\right) \left[
                                      \gamma \nabla_\theta Q^\theta(s, a^*)
                                    - \nabla_\theta Q^\theta(s, a) \right]
\end{align*}
where $a^* = \argmax_{a \in A} Q^\theta(s, a)$ is the optimal action according
to $Q^\theta$. Unfortunately, it is not possible to obtain an unbiased estimate
of $Q^\theta(s, a) Q^\theta(s, a^*)$ using one sample $(s, a, r, s')$. Instead,
Q-learning assumes that $y$ is constant and approximates the gradient as
\begin{align}
    \nabla_\theta \mathcal{L}_\text{approx} &= -\left(y - Q^\theta(s, a)\right)
                                       \nabla_\theta Q^\theta(s, a).
    \label{eqn:qlearning-gradient}
\end{align}

The complete fitted Q-learning update rule is found by substituting
\cref{eqn:qlearning-gradient} into \cref{eqn:qlearning-step}:
\begin{align*}
    \theta &\gets (1 - \alpha) \theta - \alpha \left[
                y - Q^\theta(s, a)\right] \nabla Q^\theta (s, a) \\
           &\gets (1 - \alpha) \theta - \alpha \left[
                \left( r - \gamma Q^\theta(s, a^*) \right)
                - Q^\theta(s, a)\right] \nabla Q^\theta (s, a).
\end{align*}

\subsection{Bellman Residual Method}
Q-learning as described above does not implement gradient descent and, thus, is
not guaranteed to converge to a local minimum. The \emph{Bellman residual
algorithm} method avoids the approximation of \cref{eqn:qlearning-gradient} by
estimating the true gradient $\nabla_\theta \mathcal{L}$. This estimation is
only unbiased if we can generate two independent successor states for taking
action $a$ in state $s$.

Generating these samples is trivial if we are able to simulate the system; i.e.
have access to a known or learned transition model. If we do not know the
transition model, then it is only possible to perform a Bellman residual update
if we postpone a backup until the same state-action pair has been observed two
or more times. This is often impossible when applying Q-learning to a real system
that has a continuous state-action space.

% TODO: Cite: http://www.leemon.com/papers/1995b.pdf

\subsection{Exploration Policies}
Unlike SARSA, which is an \emph{on-policy} method,  Q-learning is an
\emph{off-policy} method that can learn from arbitrary $(s, a, r, s')$
experiences, regardless of what policy was used to generate them. This means
that it is possible to use an \emph{exploration policy} training that
encourages the agent to visit previously unexplored regions of the state-action
space.

Two exploration policies that are commonly used with Q-learning are:
\begin{enumerate}
    \item \textbf{$\epsilon$-Greedy}. Choose the greedy action $a = \argmax_{a
        \in A} Q(s, a)$ with probability $1 - \epsilon$. Otherwise, with probability
        $\epsilon$, choose an action uniformly at random $a \sim \uniform(A)$. Higher
        values of $\epsilon$ encourage more exploration.
    \item \textbf{Boltzmann Exploration.} Choose action $a$ with probability
        \begin{align*}
            p(a) \propto \frac{\exp\left[ \beta Q(s, a) \right]}
                              {\sum_{a' \in A} \exp\left[ \beta Q(s, a') \right]},
        \end{align*}
        which is weighted towards selecting actions with higher $Q$-values. Lower
        values of $\beta$ encourage more exploration.
\end{enumerate}

\subsection{Experience Replay}
% TODO

\end{document}
